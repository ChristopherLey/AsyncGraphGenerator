{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baa6a564caa6d2",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-17T16:24:19.539175610Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import trange\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from AGG.utils import Time2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from AGG.extended_typing import collate_graph_samples\n",
    "from Datasets.SparseSinusoid.datareader import SinusoidData, SinusoidDataset\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139aefb-dc5e-45ca-9cf9-8b86eb74ff3a",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250b9e218e80e10",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sinusoid_data = SinusoidData(\n",
    "    context_length=30,\n",
    "    t_end=1000,\n",
    "    fs=5,\n",
    "    f=0.1,\n",
    "    sparsity=0.80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f6335f0ac6492",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_reader = SinusoidDataset(\n",
    "    train=True,\n",
    "    train_split=0.8,\n",
    "    sinusoid_data=sinusoid_data\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_reader,\n",
    "    shuffle=True,\n",
    "    batch_size=2000,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_graph_samples,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "val_reader = SinusoidDataset(\n",
    "    train=False,\n",
    "    train_split=0.8,\n",
    "    sinusoid_data=sinusoid_data\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_reader,\n",
    "    shuffle=False,\n",
    "    batch_size=2000,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_graph_samples,\n",
    "    persistent_workers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecef79e0bfe81b",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "experiment = datetime.now().strftime(\"%d-%m_%H:%M:%S\")\n",
    "print(f'Log for {experiment=}')\n",
    "dir_path = Path(f'./logs/{experiment}')\n",
    "if save:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ceda75a83c5eb",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(sinusoid_data.t, sinusoid_data.x, label='sinusoid')\n",
    "plt.plot(sinusoid_data.training_samples_t, sinusoid_data.training_samples, 'o', label=f'training samples: {sinusoid_data.training_samples.shape[0]}')\n",
    "plt.plot(sinusoid_data.target_samples_t, sinusoid_data.target_samples, 'o', label=f'target samples: {sinusoid_data.target_samples.shape[0]}')\n",
    "plt.legend()\n",
    "if save:\n",
    "    plt.savefig(dir_path / 'data_samples.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44674a562f12f6a",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sample = train_reader[1]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sample.time, sample.node_features, 'x', label='sinusoid')\n",
    "plt.plot(sample.target.time, sample.target.features, 'o', label='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361af9a124d90f2",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class GraphAttentionGenerator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            query_dim: int,\n",
    "            key_dim: int,\n",
    "            value_dim: int,\n",
    "            hidden_dim: int,\n",
    "            num_heads: int = 2,\n",
    "            dropout: float = 0.1,\n",
    "            negative_slope: float = 0.2,\n",
    "            is_concat: bool = True\n",
    "    ):\n",
    "        super(GraphAttentionGenerator, self).__init__()\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.is_concat = is_concat\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if is_concat:\n",
    "            assert hidden_dim % num_heads == 0\n",
    "            self.hidden_dim = hidden_dim // num_heads\n",
    "        else:\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.linear_query = nn.Linear(query_dim, self.hidden_dim * num_heads)\n",
    "        self.linear_key = nn.Linear(key_dim, self.hidden_dim * num_heads)\n",
    "        self.linear_value = nn.Linear(value_dim, self.hidden_dim * num_heads)\n",
    "        self.attention = nn.Linear(self.hidden_dim, 1, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        # key: [batch_size, num_nodes, key_dim]\n",
    "        # value: [batch_size, num_nodes, value_dim]\n",
    "        batch_size, num_nodes, _ = key.shape\n",
    "        if len(query.shape) != 3:\n",
    "            # query: [batch_size, query_dim]\n",
    "            query = query.unsqueeze(1)\n",
    "        # query: [batch_size, num_nodes, query_dim]\n",
    "        h_query = self.linear_query(query)\n",
    "        # h_query: [batch_size, num_nodes, hidden_dim]\n",
    "\n",
    "        # Create Heads\n",
    "        h_query = h_query.view(batch_size, 1, self.num_heads, self.hidden_dim)\n",
    "        # h_query: [batch_size, 1, num_heads, hidden_dim]\n",
    "        h_key = self.linear_key(key).view(batch_size, num_nodes, self.num_heads, self.hidden_dim)\n",
    "        # h_key: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "\n",
    "        # Duplicate query\n",
    "        h_query = h_query.repeat(1, num_nodes, 1, 1)\n",
    "        # h_query: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "\n",
    "        # Add query and key\n",
    "        h = h_query + h_key\n",
    "        # h: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "\n",
    "        e = self.leaky_relu(h)\n",
    "        e = self.attention(e)\n",
    "        # e: [batch_size, num_nodes, num_heads, 1]\n",
    "        e = e.squeeze(-1)\n",
    "        # e: [batch_size, num_nodes, num_heads]\n",
    "        e = torch.softmax(e, dim=1)\n",
    "        e = self.dropout(e)\n",
    "        # e: [batch_size, num_nodes, num_heads]\n",
    "        value = self.linear_value(value).view(batch_size, num_nodes, self.num_heads, self.hidden_dim)\n",
    "        # value: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "        \n",
    "        scored_nodes = torch.einsum(\"bjh,bjhf->bhf\", e, value)\n",
    "        # scored_value: [batch_size, num_heads, hidden_dim]\n",
    "        if self.is_concat:\n",
    "            scored_nodes = scored_nodes.reshape(batch_size, -1)\n",
    "        else:\n",
    "            scored_nodes = torch.mean(scored_nodes, dim=1)\n",
    "        h_prime = self.non_linearity(scored_nodes)\n",
    "        return h_prime, e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179f893b8e9c657",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            query_dim: int,\n",
    "            key_dim: int,\n",
    "            value_dim: int,\n",
    "            hidden_dim: int,\n",
    "            num_heads: int = 2,\n",
    "            dropout: float = 0.1,\n",
    "            negative_slope: float = 0.2,\n",
    "            is_concat: bool = True,\n",
    "            share_weights: bool = True\n",
    "    ):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.query_dim = query_dim\n",
    "        self.key_dim = key_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.is_concat = is_concat\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if is_concat:\n",
    "            assert hidden_dim % num_heads == 0\n",
    "            self.hidden_dim = hidden_dim // num_heads\n",
    "        else:\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.query_projection = nn.Linear(query_dim, self.hidden_dim * num_heads)\n",
    "        if share_weights:\n",
    "            self.key_projection = self.query_projection\n",
    "        else:\n",
    "            self.key_projection = nn.Linear(key_dim, self.hidden_dim * num_heads)\n",
    "        self.linear_value = nn.Linear(value_dim, self.hidden_dim * num_heads)\n",
    "        self.attention = nn.Linear(self.hidden_dim, 1, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        # key: [batch_size, num_nodes, key_dim]\n",
    "        batch_size, num_nodes, _ = key.shape\n",
    "        # query: [batch_size, num_nodes, query_dim]\n",
    "        h_query = self.query_projection(query)\n",
    "        # h_query: [batch_size, num_nodes, hidden_dim]\n",
    "        h_key = self.key_projection(key)\n",
    "        # h_key: [batch_size, num_nodes, hidden_dim]\n",
    "\n",
    "        # Create Heads\n",
    "        h_query = h_query.view(batch_size, num_nodes, self.num_heads, self.hidden_dim)\n",
    "        # h_query: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "        h_key = h_key.view(batch_size, num_nodes, self.num_heads, self.hidden_dim)\n",
    "        # h_key: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "\n",
    "        # Duplicate query\n",
    "        h_query_repeated = h_query.repeat(1, num_nodes, 1, 1)\n",
    "        # h_query: [batch_size, num_nodes*num_nodes, num_heads, hidden_dim]\n",
    "        # Duplicate key\n",
    "        h_key_repeated = h_key.repeat_interleave(num_nodes, dim=1)\n",
    "        # h_key: [batch_size, num_nodes*num_nodes, num_heads, hidden_dim]\n",
    "        \n",
    "        # Add query and key\n",
    "        h = h_query_repeated + h_key_repeated\n",
    "        # h: [batch_size, num_nodes*num_nodes, num_heads, hidden_dim]\n",
    "        h = h.view(batch_size, num_nodes, num_nodes, self.num_heads, self.hidden_dim)\n",
    "        # h: [batch_size, num_nodes, num_nodes, num_heads, hidden_dim]\n",
    "        \n",
    "        e = self.leaky_relu(h)\n",
    "        e = self.attention(e)\n",
    "        # e: [batch_size, num_nodes, num_nodes, num_heads, 1]\n",
    "        e = e.squeeze(-1)\n",
    "        # e: [batch_size, num_nodes, num_nodes, num_heads]\n",
    "        e = torch.softmax(e, dim=2)\n",
    "        e = self.dropout(e)\n",
    "        # e: [batch_size, num_nodes, num_nodes, num_heads]\n",
    "        value = self.linear_value(value).view(batch_size, num_nodes, self.num_heads, self.hidden_dim)\n",
    "        # value: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "        \n",
    "        scored_nodes = torch.einsum(\"bijh,bjhf->bihf\", e, value)\n",
    "        # scored_value: [batch_size, num_nodes, num_heads, hidden_dim]\n",
    "        if self.is_concat:\n",
    "            scored_nodes = scored_nodes.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            scored_nodes = torch.mean(scored_nodes, dim=2)\n",
    "        h_prime = self.non_linearity(scored_nodes)\n",
    "        return h_prime, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df131035f11362b8",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class AGG(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            time_embed_dim: int, \n",
    "            feature_dim: int, \n",
    "            hidden_dim: int, \n",
    "            num_heads: int,\n",
    "            out_dim: int, \n",
    "            dropout: float = 0.1, \n",
    "            negative_slope: float = 0.2,\n",
    "            include_linear: bool = True\n",
    "    ):  \n",
    "        super(AGG, self).__init__()\n",
    "        self.time2vec = Time2Vec(time_embed_dim, include_linear=include_linear)\n",
    "        self.key_dim = time_embed_dim\n",
    "        self.query_dim = time_embed_dim\n",
    "        self.value_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.negative_slope = negative_slope\n",
    "        \n",
    "        \n",
    "        self.graph_attention_layer = GraphAttentionLayer(\n",
    "            query_dim=time_embed_dim, \n",
    "            key_dim=time_embed_dim,\n",
    "            value_dim=feature_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout, \n",
    "            negative_slope=negative_slope,\n",
    "            is_concat=True,\n",
    "            share_weights=True\n",
    "        )\n",
    "        \n",
    "        self.graph_attention_generator = GraphAttentionGenerator(\n",
    "            query_dim=time_embed_dim, \n",
    "            key_dim=time_embed_dim,\n",
    "            value_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout, \n",
    "            negative_slope=negative_slope\n",
    "        )\n",
    "        self.prediction = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, value, key_time, query_time, device: str = 'cpu'):\n",
    "        value = value.unsqueeze(-1).to(device)\n",
    "        key_time = key_time.unsqueeze(-1).to(device)\n",
    "        query_time = query_time.unsqueeze(-1).to(device)\n",
    "        \n",
    "        # Time embedding for query and key\n",
    "        query = self.time2vec(query_time)\n",
    "        key = self.time2vec(key_time)\n",
    "        \n",
    "        # Generate graph attention layer\n",
    "        h_prime, graph_attention = self.graph_attention_layer(key, key, value)\n",
    "        # h_prime = self.feed_forward(h_prime)\n",
    "        \n",
    "        # Generate conditional node\n",
    "        h_prime, graph_attention = self.graph_attention_generator(query, key, h_prime)\n",
    "        y = self.prediction(h_prime)\n",
    "        return y, graph_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b117263d4a427",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model = AGG(\n",
    "    time_embed_dim=8, \n",
    "    feature_dim=1, \n",
    "    hidden_dim=32, \n",
    "    num_heads=4,\n",
    "    out_dim=1, \n",
    "    dropout=0.1, \n",
    "    negative_slope=0.2,\n",
    "    include_linear=True\n",
    ")\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "print(f\"Number of training samples: {len(train_reader)}\")\n",
    "print(f\"Model summary: {model}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d45e343758950",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "train_RMSE = MeanSquaredError(squared=False).to(device)\n",
    "val_RMSE = MeanSquaredError(squared=False).to(device)\n",
    "epochs = 10000\n",
    "max_lr = 0.001\n",
    "min_lr = 0.0001\n",
    "warm_up = 1000\n",
    "T_max = 500\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=max_lr)\n",
    "\n",
    "total_train = np.inf\n",
    "total_val = np.inf\n",
    "lowest_loss = np.inf\n",
    "\n",
    "lr_schedule = [max_lr]\n",
    "train_loss_plot = []\n",
    "val_loss_plot = []\n",
    "val_RMSE_plot = []\n",
    "train_RMSE_plot = []\n",
    "t2v_weights = []\n",
    "t2v_bias = []\n",
    "\n",
    "with logging_redirect_tqdm():\n",
    "    prog_bar = trange(epochs, leave=True)\n",
    "    for epoch in prog_bar:\n",
    "        model.train()\n",
    "        for graph_samples in train_dataloader:\n",
    "            y_hat, total_attention = model(graph_samples.node_features, graph_samples.time, graph_samples.target.time, device=device)\n",
    "            loss = mse_loss(y_hat, graph_samples.target.features.to(device))\n",
    "            train_loss_plot.append(loss.item())\n",
    "            t2v_weights.append(model.time2vec.scale.weight.detach().cpu().tolist())\n",
    "            t2v_bias.append(model.time2vec.scale.bias.detach().cpu().tolist())\n",
    "            rmse = train_RMSE(y_hat, graph_samples.target.features.to(device))\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimiser.step()\n",
    "            prog_bar.set_description(\n",
    "                f\"current loss: {loss.item()}, lr:{lr_schedule[-1]}, train_rmse {total_train:.4f}, val_rmse {total_val:.4f}\", refresh=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_train = train_RMSE.compute()\n",
    "            train_RMSE_plot.append(total_train.item())\n",
    "            train_RMSE.reset()\n",
    "            prog_bar.set_description(\n",
    "                f\"current loss: {loss.item()}, lr:{lr_schedule[-1]}, train_rmse {total_train:.4f}, val_rmse {total_val:.4f}\", refresh=True)\n",
    "            model.eval()\n",
    "            loss_total = 0\n",
    "            for graph_samples in val_dataloader:\n",
    "                y_hat, total_attention = model(graph_samples.node_features, graph_samples.time, graph_samples.target.time, device=device)\n",
    "                loss = mse_loss(y_hat, graph_samples.target.features.to(device))\n",
    "                val_loss_plot.append(loss.item())\n",
    "                loss_total += loss.item()\n",
    "                rmse = val_RMSE(y_hat, graph_samples.target.features.to(device))\n",
    "            total_val = val_RMSE.compute()\n",
    "            val_RMSE.reset()\n",
    "            val_RMSE_plot.append(total_val.item())\n",
    "            prog_bar.set_description(\n",
    "                f\"current loss: {loss.item()}, lr:{lr_schedule[-1]}, train_rmse {total_train:.4f}, val_rmse {total_val:.4f}\",\n",
    "                refresh=True)\n",
    "            if loss_total < lowest_loss:\n",
    "                if save:\n",
    "                    torch.save(\n",
    "                        model.state_dict(), dir_path / f\"best_GAT_model_{experiment}.mdl\"\n",
    "                    )\n",
    "                    lowest_loss = total_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f6e33f8ef08b9",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(train_loss_plot, label='train loss')\n",
    "plt.plot(val_loss_plot, label='val loss')\n",
    "plt.xlabel('steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_RMSE_plot, label='train RMSE')\n",
    "plt.plot(val_RMSE_plot, label='val RMSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "t2v_weights = np.array(t2v_weights)\n",
    "t2v_bias = np.array(t2v_bias)\n",
    "for i in range(t2v_weights.shape[1]):\n",
    "    plt.plot(t2v_weights[:, i], label=f't2v[{i}] weight')\n",
    "    plt.plot(t2v_bias[:, i], label=f't2v[{i}] bias')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "80c28e7b4be9be32",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tau_query = torch.linspace(0, 1, 1000)\n",
    "tau_query.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "96780d2481d6adbf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample = 300\n",
    "node_t = val_reader.data_samples[sample].time.repeat(1000, 1)\n",
    "node = val_reader.data_samples[sample].node_features.repeat(1000, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7dbf1dbcb3b3f3c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "node.shape"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2d5bce01c68a64cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat, total_attention = model(node, node_t, tau_query, device=device)\n",
    "    y_hat = y_hat.cpu().numpy()\n",
    "    total_attention = total_attention.cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "371abef7b3ec98da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "heads = total_attention.shape[2]\n",
    "fig, ax = plt.subplots(heads, 1, figsize=(20, 10))\n",
    "for i in range(heads):\n",
    "    ax[i].imshow(total_attention[:, :, i], extent=[0, 1, 0, 1], cmap='viridis')\n",
    "    ax[i].axis('tight')\n",
    "plt.xlabel(\"node\")\n",
    "plt.ylabel(\"query\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f2b0c9091b3d579c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(tau_query, y_hat, label=f'generated signal: RMSE={min(val_RMSE_plot):.3f}')\n",
    "# plt.plot(sinusoid_data.t/sinusoid_data.t.max(), sinusoid_data.x, label='ground truth signal')\n",
    "plt.plot(val_reader.data_samples[sample].time.flatten().numpy(), \n",
    "         val_reader.data_samples[sample].node_features.flatten().numpy(), '-x', label=f'input')\n",
    "plt.plot(val_reader.data_samples[sample].target.time.flatten().numpy(), \n",
    "         val_reader.data_samples[sample].target.features.flatten().numpy(), 'ro', label=f'target')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "312e9ab0db32275",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2a1d9c2b7a8e8eb8",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
